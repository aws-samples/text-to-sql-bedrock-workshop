{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0916a3a-e402-48b7-a775-ce739e4aeaf4",
   "metadata": {},
   "source": [
    "# Introduction to Bedrock - Code Generation - Natural Language to SQL Using RAG"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "770bec8e-dd15-4b7d-8ec3-bc35baab0305",
   "metadata": {},
   "source": [
    "1.  Set up Bedrock embedding Model and LLM to Embed the table metadata.\n",
    "2.  Fetch the metadata from Athena , Get the questions what can be answered asking LLM provided schema information then Embed all the metadata in a Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8a4633",
   "metadata": {},
   "source": [
    "**Suggested SageMaker JupterLab Notebook Environment set up is as follows:**\n",
    "\n",
    "Sagemaker Image: sagemaker-distribution-cpu\n",
    "\n",
    "Kernel: Python 3\n",
    "\n",
    "Instance Type: ml.m5.large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b00409e",
   "metadata": {},
   "source": [
    "# Dependencies installations\n",
    "\n",
    "Here we will install all the required dependencies to run this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455ba14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m ensurepip --upgrade\n",
    "!pip install langchain --quiet\n",
    "!pip install jq --quiet\n",
    "!pip install faiss-cpu --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae3548d",
   "metadata": {},
   "source": [
    "**Restart your Kernel before proceeding**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c1fda97-9150-484a-8cfa-86ec9568fc61",
   "metadata": {},
   "source": [
    "## 1. Configure Bedrock embedding Model and LLM ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c70660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain.document_loaders.json_loader import JSONLoader\n",
    "from langchain.docstore.document import Document\n",
    "import json\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "import re\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c81e0a1c",
   "metadata": {},
   "source": [
    "![Alt text](content/rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07958746-7313-484a-887b-167b8d8acf31",
   "metadata": {},
   "source": [
    "#### Now let's set up our connection to the Amazon Bedrock SDK using Boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2212b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATHENA_RESULTS_S3_LOCATION = \"<workshop bucket name>\" # available in cloudformation outputs\n",
    "ATHENA_CATALOG_NAME = \"<athena catalog name>\" # available in cloudformation outputs\n",
    "DB_NAME = \"tpcds1\"\n",
    "DB_FAISS_PATH = './vectorstore/db_faiss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcd2cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_region = athena_region = boto3.session.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18230081",
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_config = Config(retries = {'max_attempts': 100})\n",
    "session = boto3.Session(region_name=bedrock_region)\n",
    "bedrock = session.client('bedrock-runtime', region_name=bedrock_region, config=retry_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce22c308-ebbf-4ef5-a823-832b7c236e31",
   "metadata": {},
   "source": [
    "### 1.2. Configure LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bf3acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm(question,modelId):\n",
    "    \n",
    "    body = ''\n",
    "    if 'titan' in modelId:\n",
    "         model_kwargs = {\"maxTokenCount\": 200, \"temperature\": 0.001} \n",
    "         input_body = dict()\n",
    "         input_body[\"inputText\"] = question\n",
    "         input_body[\"textGenerationConfig\"] = {**model_kwargs}\n",
    "         body = json.dumps(input_body)\n",
    "\n",
    "    else:\n",
    "          body = json.dumps({\n",
    "                    \"prompt\": question,\n",
    "                    \"max_tokens_to_sample\":4096,\n",
    "                    \"temperature\":0.5,\n",
    "                    \"top_k\":250,\n",
    "                    \"top_p\":0.5,\n",
    "                  }) \n",
    "    \n",
    "    accept = 'application/json'\n",
    "    contentType = 'application/json'\n",
    "\n",
    "    response = bedrock.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    print(response_body)\n",
    "    return response_body"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e9d44d0",
   "metadata": {},
   "source": [
    "### 1.3 Helper functions "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24f7187d",
   "metadata": {},
   "source": [
    "Once the LLM returns the list of questions that can be answered using the given table and column , store them locally in a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e571d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_questions_to_file(question_list_filename, table_name, table_schema, answer):\n",
    "    data_list = []\n",
    "    question_list_obj = answer\n",
    "    questions_list = question_list_obj.splitlines()\n",
    "    print(questions_list)\n",
    "    # Open the file in write mode\n",
    "    with open(question_list_filename, mode=\"w\", newline=\"\") as file:\n",
    "        for question in questions_list:\n",
    "\n",
    "            # Skip if it doesn't really have a question\n",
    "            if \"?\" not in question:\n",
    "                continue\n",
    "\n",
    "            questionSplit = re.split(r\"\\d{1,5}.||. ||- \", question, maxsplit=1)\n",
    "            print(questionSplit)\n",
    "            question = questionSplit[1]\n",
    "            data = {\n",
    "                \"tableName\": table_name,\n",
    "                \"question\": question,\n",
    "                \"tableSchema\": table_schema.lstrip(\" \"),\n",
    "            }\n",
    "            data_list.append(data)\n",
    "\n",
    "        json.dump(data_list, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a486f1a5",
   "metadata": {},
   "source": [
    " ### 1.4  Configure Bedrock Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb53761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "bedrock_embeddings = BedrockEmbeddings(client=bedrock)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d7f1d71",
   "metadata": {},
   "source": [
    "### 1.5 Create the Document object with correct metadata for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad760cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new docs with the right metadata we need for indexing\n",
    "def create_docs_with_correct_metadata(documents):\n",
    "    # We are going to return a list of new documents\n",
    "    new_docs = []\n",
    "\n",
    "    # For each document\n",
    "    for doc in documents:\n",
    "        # Get it's metadata and contents\n",
    "        metadata = doc.metadata\n",
    "        contents = json.loads(doc.page_content)\n",
    "\n",
    "        # Now calculate the new metadata that we want to add\n",
    "        new_metadata = {\n",
    "            \"tableName\": contents[\"tableName\"],\n",
    "            \"question\": contents[\"question\"],\n",
    "            \"tableSchema\": contents[\"tableSchema\"],\n",
    "        }\n",
    "\n",
    "        # Print out the new metadata for our documents\n",
    "        # print(new_metadata)\n",
    "\n",
    "        new_docs.append(\n",
    "            Document(page_content=new_metadata[\"question\"], metadata=new_metadata)\n",
    "        )\n",
    "\n",
    "    return new_docs\n",
    "\n",
    "def load_json_file(filename):\n",
    "    loader = JSONLoader(file_path=filename, jq_schema=\".[]\", text_content=False)\n",
    "\n",
    "    # This is our internal Langchain document data structure\n",
    "    docs = loader.load()\n",
    "    return docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a1d76a9",
   "metadata": {},
   "source": [
    "### 1.6 Below prompt is used to get the questions in natural language and it calls all the helper function to embed the questions and table metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4367048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function asks the LLM to inspect a table schema, generate some questions which could be answered\n",
    "# by that schema, and then it stores those questions to file, loads them all into a single vectorDB\n",
    "def add_new_table(schema, table_name,model_id,is_incremental):\n",
    "    \"\"\"\n",
    "    :schema         :   \n",
    "    :table_name     :\n",
    "    :model_id       :\n",
    "    :is_incremental :\n",
    "    \"\"\"\n",
    "    print(f\"Adding table {table_name} with schema {schema}\")\n",
    "    \n",
    "    question = f\"\"\"\n",
    "    \\n\\nHuman: \n",
    "    only return the a bulleted numbered list of unique and detailed questions that could be answered by this table called {table_name} with schema:\n",
    "    {schema}.\n",
    "    Instructions:\n",
    "        Use natural language descriptions only.\n",
    "        Do not use SQL.\n",
    "        Produced a varied list of questions, but the questions should be unique and detailed.\n",
    "        The questions should be in a format that is easy to understand and answer.\n",
    "        Ask about as much of the information in the table as possible.\n",
    "        You can ask about more than one aspect of the data at a time.\n",
    "        Qustions should begin with, 'What', 'Which', 'How', 'When' or 'Can'. Use variable names. \n",
    "        The questions should use relevant buisness vocabularly and terminology only. \n",
    "        Do not use column names in your output - use relevant natural language descriptions only. \n",
    "        Do not output any numeric values.\n",
    "        Output questions starting with bulleted numbered list. \n",
    "         \n",
    "        \\n Questions: 1.\n",
    "        \\n Assistant:\n",
    "        \"\"\"\n",
    "       \n",
    "\n",
    "    response = ask_llm(question,model_id)\n",
    "    answer = response['completion']\n",
    "    question_list_filename = f\"../questionList{table_name}.json\"\n",
    "\n",
    "    # # Get rid of anything before the 1.\n",
    "    # if re.match(r\"^[^\\d+]\\. \", answer) and re.search(r\"\\d+\\. \", answer):\n",
    "    #     answer = \"1. \" + answer.split(\"1. \")[1]\n",
    "    # else:\n",
    "    #     answer = \"1. \" + answer\n",
    "\n",
    "    print(\n",
    "        f\"Writing questions to {question_list_filename}, with schema {schema}, with table name {table_name} and answer {answer}.\\n\\n\"\n",
    "    )\n",
    "\n",
    "    write_questions_to_file(question_list_filename, table_name, schema, answer)\n",
    "    docs = load_json_file(question_list_filename)\n",
    "    docs = create_docs_with_correct_metadata(docs)\n",
    "    new_questions = FAISS.from_documents(docs, bedrock_embeddings)\n",
    "    db_exists = True if os.path.exists(f\"{DB_FAISS_PATH}/index.faiss\") else False\n",
    "    # Add new tables\n",
    "    if is_incremental and db_exists:\n",
    "            question_db = FAISS.load_local(DB_FAISS_PATH, bedrock_embeddings)\n",
    "            question_db.merge_from(new_questions)\n",
    "            question_db.save_local(DB_FAISS_PATH)\n",
    "\n",
    "    # Load for the first time\n",
    "    else:\n",
    "        print(f\"is_incremental set to {str(is_incremental)} and/or no vector db found. Creating...\")\n",
    "        new_questions.save_local(DB_FAISS_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49fc5e86",
   "metadata": {},
   "source": [
    " ### 2.  Fetch tpc-ds dataset Tables and Columns information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f4afd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import MetaData\n",
    "from sqlalchemy import create_engine\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "\n",
    "\n",
    "def get_sqlalchemy_athena(database,catalog,s3stagingathena,region):\n",
    "\n",
    "    athena_connection_str = f'awsathena+rest://:@athena.{region}.amazonaws.com:443/{database}?s3_staging_dir={s3stagingathena}&catalog_name={catalog}'\n",
    "    # Create Athena engine\n",
    "    athena_engine = create_engine(athena_connection_str) \n",
    "    return athena_engine\n",
    "\n",
    "\n",
    "def get_tpc_ds_dataset(database,catalog,s3stagingathena,region):\n",
    "# Reflect db schema\n",
    "    \n",
    "    column_table  =[]\n",
    "    columns_str= ''\n",
    "    table_name = ''\n",
    "    metadata = MetaData()\n",
    "    engine = get_sqlalchemy_athena(database,catalog,s3stagingathena,region)\n",
    "    metadata.reflect(bind=engine)\n",
    "\n",
    "    # Get list of table names\n",
    "    print(metadata.tables.keys()) \n",
    "\n",
    "    # Loop through tables\n",
    "    for table in metadata.tables:\n",
    "        print(f\"Table: {table}\")\n",
    "        table_name= table\n",
    "        columns_str= ''\n",
    "        tuple = ''\n",
    "        print(f\"Schema: {metadata.tables[table].schema}\")\n",
    "        print(f\"Columns: {metadata.tables[table].columns.keys()}\")                \n",
    "\n",
    "        for column in metadata.tables[table].columns.keys():\n",
    "                columns_str=columns_str+f'{column}'+\"|\"  \n",
    "                \n",
    "        tuple = columns_str,table_name\n",
    "        column_table.append(tuple)      \n",
    "    return  column_table      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46f4c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpc_ds = get_tpc_ds_dataset(DB_NAME,ATHENA_CATALOG_NAME,ATHENA_RESULTS_S3_LOCATION,athena_region)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbb3a532",
   "metadata": {},
   "source": [
    "### Embed all the questions and metadata to a vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2cdf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'anthropic.claude-v2'\n",
    "for x in tpc_ds:\n",
    "    print(x)\n",
    "    add_new_table(x[0],x[1],model_id,True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55a2dff5",
   "metadata": {},
   "source": [
    "### We can now execute the inference notebook to Ask a question"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
