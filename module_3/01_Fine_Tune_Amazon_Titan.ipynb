{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning for Text-to-SQL on Amazon Bedrock\n",
    "Use of Bedrock Fine-tuning to improve Text-to-SQL accuracy.\n",
    "\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested SageMaker Environment\n",
    "Sagemaker Image: sagemaker-distribution-cpu\n",
    "\n",
    "Kernel: Python 3\n",
    "\n",
    "Instance Type: ml.m5.large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "1. [Overview of Building Training Data Frame](#step-1-overview-of-building-training-data-frame)\n",
    "1. [Download the Spider Dataset](#step-2-download-the-spider-data-set)\n",
    "1. [Install Dependencies](#step-3-install-dependencies)\n",
    "1. [Training Data Helper Functions](#step-4-training-data-helper-functions)\n",
    "1. [Build Training Data Frames and Save](#step-5-build-training-data-frames-and-save)\n",
    "1. [Format Training Data for Amazon Bedrock Fine-Tuning](#step-6-format-training-data-for-amazon-bedrock-fine-tuning)\n",
    "1. [Configure Constraints for Fine-Tuning](#step-7-configure-constraints-for-fine-tuning)\n",
    "1. [Generate Files Formatted for Bedrock Fine-Tuning](#step-8-generate-files-formatted-for-bedrock-fine-tuning)\n",
    "1. [Validate Training File Structure](#step-9-validate-training-file-structure)\n",
    "1. [Upload Training Data](#step-10-upload-training-data)\n",
    "1. [Kick Off a Fine-Tuning Job from the SDK](#step-11-kick-off-a-fine-tuning-job-from-the-sdk)\n",
    "1. [Created Bedrock Provisioned Model](#step-12-create-provisioned-model)\n",
    "1. [Setup for Benchmarking Model Performance](#step-13-setup-for-benchmarking-model-performance)\n",
    "1. [Run Off-the-Shelf Models](#step-14-run-off-the-shelf-models)\n",
    "1. [Run Our Titan Fine-Tuned Model](#step-15-run-on-our-titan-fine-tuned-model)\n",
    "1. [Set up for Model Performance](#step-16-set-up-for-model-performance)\n",
    "1. [Evaluate Models](#step-17-evaluate-models)\n",
    "1. [Analyze Results](#step-18-analyze-results)\n",
    "1. [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Objective\n",
    "The following notebook will walk you through setting up and extracting the right information from your source databases, as well as how to kick off a fine-tuning job on Amazon Bedrock.\n",
    "\n",
    "To get a good understanding of our performance, we'll run our benchmarks against the actual SQL databases provided in the [Spider Dataset](https://github.com/taoyds/spider/tree/master).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Approach to the Text-to-SQL Problem\n",
    "The [SQL-PALM](https://arxiv.org/abs/2010.02840) publication was one of the very influential papers in the natural language to SQL space.\n",
    "We therefore want to recreate the training set for the [Spider Dataset](https://github.com/taoyds/spider/tree/master) with the same schema, foreign and primary keys information as we have found in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools\n",
    "Amazon Bedrock SDK, Pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Overview of Building Training Data Frame\n",
    "\n",
    "Before we can start training, we need to set up our datasets. Therefore, we will bring in the original JSON files found in the data repository and define a few helper functions to assist us with wrangling the JSON files into a coherent data frame.\n",
    "\n",
    "After that, we're going to concatenate all the training data files to obtain one large training dataset. We will also examine the table information provided to us by the original Spider dataset. Once we're done constructing the query data frame, we will start constructing the schema from the table information, as well as constructing the primary and foreign keys.\n",
    "\n",
    "The SQL PALM paper has a different format than the regular DDL description of tables. We can observe that the DDL format, on average, uses more tokens to describe the same table, but might also be found more frequently in training datasets that have been ingested into a large language model training. For our example, we are going to stick to the SQL PALM format.\n",
    "\n",
    "If the DDL format is new for you, feel free to follow the [LINK](https://www.ibm.com/docs/en/i/7.2?topic=programming-data-definition-language) for a brief intro.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Download the Spider Data Set\n",
    "1. Visit [this link](https://drive.google.com/u/0/uc?id=1iRDVHLr4mX2wQKSgA9J8Pire73Jahh0m&export=download) and click 'Download anyway'\n",
    "2. Once the download to your computer is complete, upload to the JupyterLab file explorer on the left. This can take a few minutes and should say `Uploading...` below in the status bar of JupyterLab.\n",
    "3. [Open a terminal session](https://jupyterlab.readthedocs.io/en/stable/user/terminal.html), navigate to your uploaded file, and use `unzip` to unzip the file. For example: `sagemaker-user@default:~/module_3$ unzip spider.zip`\n",
    "4. Verify the `spider` folder has been extracted, and update the below file path to point its location. This will be used throughout this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spider_folder = '/home/sagemaker-user/text-to-sql-bedrock-workshop/module_3/spider'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "from random import randint\n",
    "import logging\n",
    "from botocore.exceptions import ClientError\n",
    "from botocore.config import Config\n",
    "import os\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import pickle\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import random\n",
    "import sqlite3\n",
    "\n",
    "# print the full string, no truncation\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# set s3 bucket:\n",
    "S3_BUCKET_NAME = \"<AthenaResultsS3Location>\" # Can be found in CloudFormation outputs\n",
    "FINE_TUNING_JOB_ROLE_ARN = \"<BedrockFineTuningJobRole>\" # can be found in the cloudformation outputs under BedrockFineTuningJobRole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Training Data Helper Functions\n",
    "These helper functions will assist with reading the .json files and constructing the query data sets appropriately for fine-tuning our Titan model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(file_name):\n",
    "    \"\"\"\n",
    "    Reads a JSON file and returns its contents as a dictionary.\n",
    "\n",
    "    Args:\n",
    "    file_name (str): The name of the JSON file to read.\n",
    "\n",
    "    Returns:\n",
    "    dict: The contents of the JSON file as a dictionary.\n",
    "    \"\"\"\n",
    "    with open(file_name) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def construct_queries(queries):\n",
    "    \"\"\"\n",
    "    Reads a JSON file containing queries and returns a pandas DataFrame with the db_id, query, and question.\n",
    "\n",
    "    Args:\n",
    "    queries (str): The path to the JSON file containing the queries.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the db_id, query, and question.\n",
    "    \"\"\"\n",
    "    queries = read_json_file(queries)\n",
    "\n",
    "    query_df = pd.DataFrame(columns=[\"db_id\", \"query\", \"question\"])\n",
    "    for idx, _ in enumerate(queries):\n",
    "        db_id = queries[idx][\"db_id\"]\n",
    "        query = queries[idx][\"query\"]\n",
    "        question = queries[idx][\"question\"]\n",
    "\n",
    "        query_df.loc[idx] = [db_id, query, question]\n",
    "\n",
    "    return query_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll combine the two training data sets offered by Spider, into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_train_spider = construct_queries(\n",
    "    f\"{spider_folder}/train_spider.json\"\n",
    ")\n",
    "query_train_other = construct_queries(\n",
    "    f\"{spider_folder}/train_others.json\"\n",
    ")\n",
    "\n",
    "# Concatenate DataFrames\n",
    "query_train = pd.concat([query_train_spider, query_train_other], ignore_index=True)\n",
    "\n",
    "# Add index column to query_train_other\n",
    "query_train.insert(0, \"index\", range(len(query_train)))\n",
    "\n",
    "query_dev = construct_queries(f\"{spider_folder}/dev.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More Helper functions for preparing our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_schema(table):\n",
    "    \"\"\"\n",
    "    Constructs a string representation of the schema for a given table.\n",
    "\n",
    "    Args:\n",
    "    table (dict): A dictionary containing information about a table in the Spider dataset.\n",
    "\n",
    "    Returns:\n",
    "    str: A string representation of the schema for the given table.\n",
    "    \"\"\"\n",
    "    no_tables = len(table[\"table_names_original\"])\n",
    "    table_names_original = table[\"table_names_original\"]\n",
    "\n",
    "    Schema = f\"[Schema (values) (types)]: | {table['db_id']} | \"\n",
    "    for i in range(no_tables):\n",
    "        Schema += f\" {table_names_original[i]} : \"\n",
    "\n",
    "        tableCols = [x[1] for x in table[\"column_names_original\"] if x[0] == i]\n",
    "        for j in range(len(tableCols)):\n",
    "            if j != len(tableCols) - 1:\n",
    "                Schema += f\"{tableCols[j].lower()} ({table['column_types'][j]}) , \"\n",
    "            elif j == len(tableCols) - 1 and i != no_tables - 1:\n",
    "                Schema += f\"{tableCols[j].lower()} ({table['column_types'][j]}) |\"\n",
    "            else:\n",
    "                Schema += f\"{tableCols[j].lower()} ({table['column_types'][j]});\"\n",
    "\n",
    "    return Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_primary_keys(table):\n",
    "    \"\"\"\n",
    "    Constructs a string representation of the primary keys for a given table.\n",
    "\n",
    "    Args:\n",
    "    table (dict): A dictionary containing information about a table in the Spider dataset.\n",
    "\n",
    "    Returns:\n",
    "    str: A string representation of the primary keys for the given table.\n",
    "    \"\"\"\n",
    "    primary_keys = \"[Primary Keys]: \"\n",
    "    for idx, key in enumerate(table[\"primary_keys\"]):\n",
    "        table_name = table[\"table_names_original\"][idx].lower()\n",
    "        primary_key = table[\"column_names_original\"][key][1].lower()\n",
    "        if idx != len(table[\"primary_keys\"]) - 1:\n",
    "            primary_keys += f\"{table_name} : {primary_key}, \"\n",
    "        else:\n",
    "            primary_keys += f\"{table_name} : {primary_key}\"\n",
    "    return primary_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_foreign_keys(table):\n",
    "    \"\"\"\n",
    "    Constructs a string representation of the foreign keys for a given table.\n",
    "\n",
    "    Args:\n",
    "    table (dict): A dictionary containing information about a table in the Spider dataset.\n",
    "\n",
    "    Returns:\n",
    "    str: A string representation of the foreign keys for the given table.\n",
    "    \"\"\"\n",
    "    foreign_keys = \"[Foreign Keys]: \"\n",
    "    for i in range(len(table[\"foreign_keys\"])):\n",
    "        fk1 = table[\"foreign_keys\"][i][0]\n",
    "        fk2 = table[\"foreign_keys\"][i][1]\n",
    "\n",
    "        fk1_name = table[\"column_names_original\"][fk1][1].lower()\n",
    "        fk1_table_idx = table[\"column_names_original\"][fk1][0]\n",
    "        fk1_table = table[\"table_names_original\"][fk1_table_idx].lower()\n",
    "\n",
    "        fk2_name = table[\"column_names_original\"][fk2][1].lower()\n",
    "        fk2_table_idx = table[\"column_names_original\"][fk2][0]\n",
    "        fk2_table = table[\"table_names_original\"][fk2_table_idx].lower()\n",
    "\n",
    "        if i != len(table[\"foreign_keys\"]) - 1:\n",
    "            foreign_keys += f\"{fk1_table} : {fk1_name} = {fk2_table} : {fk2_name} | \"\n",
    "        else:\n",
    "            foreign_keys += f\"{fk1_table} : {fk1_name} = {fk2_table} : {fk2_name}\"\n",
    "\n",
    "    return foreign_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_table_df(tables_path):\n",
    "    \"\"\"\n",
    "    Constructs a pandas dataframe containing information about tables in the Spider dataset.\n",
    "\n",
    "    Args:\n",
    "    tables_path (str): The path to the tables.json file in the Spider dataset.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A dataframe containing information about tables in the Spider dataset.\n",
    "    \"\"\"\n",
    "    tables = read_json_file(tables_path)\n",
    "    table_df = pd.DataFrame(columns=[\"db_id\", \"schema\", \"primary_keys\", \"foreign_keys\"])\n",
    "    for idx, table in enumerate(tables):\n",
    "        db_id = table[\"db_id\"]\n",
    "        schema = construct_schema(table)\n",
    "        primary_keys = construct_primary_keys(table)\n",
    "        foreign_keys = construct_foreign_keys(table)\n",
    "        table_df.loc[idx] = [db_id, schema, primary_keys, foreign_keys]\n",
    "    return table_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Build training data frames and save\n",
    "Here we build our training data frame, verify its the correct shape, and save as a `.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "table_df = construct_table_df(f\"{spider_folder}/tables.json\")\n",
    "train_df = pd.merge(query_train, table_df, on=\"db_id\", how=\"inner\")\n",
    "dev_df = pd.merge(query_dev, table_df, on=\"db_id\", how=\"inner\")\n",
    "\n",
    "assert len(train_df) == len(query_train_other) + len(query_train_spider)\n",
    "assert len(dev_df) == len(query_dev)\n",
    "\n",
    "train_df.to_csv(\"train_master.csv\", index=False)\n",
    "dev_df.to_csv(\"dev_master.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Format Training Data for Amazon Bedrock Fine-Tuning\n",
    "Before we can use the data, that we have put in our data frame, we first need to wrangle the data to align to a supervised fine-tuning approach.\n",
    "\n",
    "This means that we need to construct an instruction set with context, as well as the desired output. Amazon Bedrock fine-tuning will then run a supervised fine-tuning on our data with the selected model.\n",
    "\n",
    "Amazon Bedrock expects our data in the `.jsonl` format.\n",
    "Please be aware that the keys in the `.jsonl` data that we need to dump might be different from model to model.\n",
    "For Amazon Titan, we're looking for a JSON line entry that has the following keys: `input` and `output`. The `input` is the instruction that describes the task, as well as any context you want to give to the model. Furthermore, examples could be placed here, which we call few-shot learning.\n",
    "\n",
    "When we fine-tune large language models on instruction datasets,\n",
    "the clarity of the instruction can help us decrease the loss in the training process quicker than having no instructions.\n",
    "Furthermore, you can use \"few-shot examples\", which means giving the model an example of your desired output, which can help to further reduce the training loss quicker.\n",
    "\n",
    "However, you want to vary your examples throughout your training dataset. Otherwise, the you might overfit to this example, which could have negative effects on your inference performance.\n",
    "\n",
    "The second part of every `.jsonl` entry - the `output` - is the desired output from the model. Many models follow the prompt technique that we call \"putting words in their mouths,\" so if you know that you are always going to be running a `SELECT` statement, you could trigger the model, by adding the `SELECT` already at the end of the instruction. However, for our example, we're not going to do that. We simply want to create a SQL statement that will be the only output from the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def template_dataset_titan(\n",
    "    sample: dict, return_jsonl: bool = True, SQL_flavour=\"SQLite\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generates a JSON Lines string from a sample dictionary.\n",
    "\n",
    "    Args:\n",
    "        sample (dict): A dictionary containing the sample data.\n",
    "\n",
    "    Returns:\n",
    "        str: A JSON Lines string representing the sample data.\n",
    "    \"\"\"\n",
    "    SystemPrompt = f\"Your tasks converting text into SQL statements. We will first give the dataset schema and column types, primary keys and foreign keys and then ask a question in text. You are asked to generate SQL statement in {SQL_flavour}.\\n \"\n",
    "    prompt_template = (\n",
    "        SystemPrompt\n",
    "        + '{Schema}\"\\n{primary_keys}\"\\n{foreign_keys}\"\\nAnswer the following question with a SQL Statement:{Question}\\n[SQL]:\\n'\n",
    "    )\n",
    "\n",
    "    question = sample[\"question\"]\n",
    "    schema = sample[\"schema\"]\n",
    "    primary_keys = sample[\"primary_keys\"]\n",
    "    foreign_keys = sample[\"foreign_keys\"]\n",
    "    sql_query = sample[\"query\"]\n",
    "\n",
    "    input_text = prompt_template.format(\n",
    "        Question=question,\n",
    "        Schema=schema,\n",
    "        primary_keys=primary_keys,\n",
    "        foreign_keys=foreign_keys,\n",
    "    )\n",
    "    output_text = sql_query\n",
    "    if return_jsonl:\n",
    "        json_line = json.dumps({\"input\": input_text, \"output\": output_text})\n",
    "        return json_line\n",
    "    else:\n",
    "        return {\"input\": input_text, \"output\": output_text}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets inspect if everything is as we would expect it, by looking at an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_dataset_titan(train_df.iloc[0].to_dict(), return_jsonl=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Configure Constraints for Fine-Tuning\n",
    "\n",
    "When fine-tuning on Amazon Titan models on Amazon Bedrock, we must adhere to several constraints.\n",
    "Firstly, we have to set the maximum input and output characters to roughly 12,000 characters.\n",
    "\n",
    "The total number of training records we can supply is 10,000 and the maximum validation records are 1,000 examples.\n",
    "\n",
    "Therefore, we are going to generate our training datasets according to those constraints.\n",
    "\n",
    "We will do so with the help of a few functions that we will be defining in the following cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "constrains = {\n",
    "    \"maxInputChars\": 12288,\n",
    "    \"maxOutputChars\": 12288,\n",
    "    \"maxTotalChars\": 24576,\n",
    "    \"TrainingRecords\": 10000,\n",
    "    \"ValidationRecords\": 1000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_jsonl_file(\n",
    "    dataset: pd.DataFrame,\n",
    "    savepath: str,\n",
    "    enforce_titan_constrains: bool = True,\n",
    "    constrains: dict = None,\n",
    "    train: bool = True,\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Generates a JSON Lines file from a given dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): A pandas DataFrame containing the dataset.\n",
    "        savepath (str): The path to save the generated JSON Lines file.\n",
    "        enforce_titan_constrains (bool, optional): Whether to enforce Titan constraints. Defaults to True.\n",
    "        constrains (dict, optional): A dictionary containing the constraints to enforce. Defaults to None.\n",
    "        train (bool, optional): Whether the dataset is for training. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of indices of dropped records.\n",
    "    \"\"\"\n",
    "    databaseList = []\n",
    "    if enforce_titan_constrains != True:\n",
    "        json_output = []\n",
    "        for idx in range(0, dataset.shape[0]):\n",
    "            sample = dataset.iloc[idx]\n",
    "            json_line = template_dataset_titan(sample)\n",
    "            json_output.append(json_line)\n",
    "            databaseList.append(sample[\"db_id\"])\n",
    "        with open(savepath, \"w\") as f:\n",
    "            f.write(\"\\n\".join(json_output))\n",
    "\n",
    "    elif enforce_titan_constrains == True:\n",
    "        json_output = []\n",
    "        droppedIdx = []\n",
    "        if train == True:\n",
    "            maxRecords = constrains[\"TrainingRecords\"]\n",
    "        else:\n",
    "            maxRecords = constrains[\"ValidationRecords\"]\n",
    "\n",
    "        maxTotalChars = constrains[\"maxTotalChars\"]\n",
    "        maxOutputChars = constrains[\"maxOutputChars\"]\n",
    "        maxInputChars = constrains[\"maxInputChars\"]\n",
    "\n",
    "        droppedRecords = 0\n",
    "\n",
    "        for idx in range(0, min(maxRecords, dataset.shape[0])):\n",
    "            sample = dataset.iloc[idx]\n",
    "            json_line = template_dataset_titan(sample, return_jsonl=False)\n",
    "            if (\n",
    "                len(json_line[\"input\"]) <= maxInputChars\n",
    "                and len(json_line[\"output\"]) <= maxOutputChars\n",
    "                and len(json_line[\"input\"]) + len(json_line[\"output\"]) <= maxTotalChars\n",
    "            ):\n",
    "                json_line = json.dumps(json_line)\n",
    "                json_output.append(json_line)\n",
    "                databaseList.append(sample[\"db_id\"])\n",
    "            else:\n",
    "                print(f\"Sample at index {idx} dropped due to constraints\")\n",
    "                droppedRecords += 1\n",
    "        print(f\"Total records dropped: {droppedRecords}\")\n",
    "\n",
    "        with open(savepath, \"w\") as f:\n",
    "            f.write(\"\\n\".join(json_output))\n",
    "\n",
    "        return droppedIdx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Generate Files Formatted for Bedrock Fine-Tuning\n",
    "Now with our helper functions described above we'll generate the necessary `.jsonl` files to use in our job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory for output data sets\n",
    "!mkdir output_datasets\n",
    "output_datasets_dir = \"./output_datasets\"\n",
    "\n",
    "# set output paths for training files\n",
    "train_path_local = (\n",
    "    f\"{output_datasets_dir}/train_titan.jsonl\"\n",
    ")\n",
    "validation_path_local = (\n",
    "    f\"{output_datasets_dir}/eval_titan.jsonl\"\n",
    ")\n",
    "benchmark_path_local = (\n",
    "    f\"{output_datasets_dir}/validation_titan.jsonl\"\n",
    ")\n",
    "\n",
    "# generate training files in their respective locations\n",
    "dropped_records_train = generate_jsonl_file(\n",
    "    train_df,\n",
    "    train_path_local,\n",
    "    constrains=constrains,\n",
    "    train=True,\n",
    "    enforce_titan_constrains=True,\n",
    ")\n",
    "\n",
    "dropped_records_dev = generate_jsonl_file(\n",
    "    dev_df,\n",
    "    validation_path_local,\n",
    "    constrains=constrains,\n",
    "    train=False,\n",
    "    enforce_titan_constrains=True,\n",
    ")\n",
    "\n",
    "dropped_records_dev = generate_jsonl_file(\n",
    "    dev_df,\n",
    "    benchmark_path_local,\n",
    "    constrains=constrains,\n",
    "    train=False,\n",
    "    enforce_titan_constrains=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save in feather format for use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Intermediary save to feather format\n",
    "dev_df.to_feather(\n",
    "    f\"{output_datasets_dir}/dev_df.feather\"\n",
    ")\n",
    "train_df.to_feather(\n",
    "    f\"{output_datasets_dir}/train_df.feather\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Validate training file structure\n",
    "We'll do this by creating a `test_jsonl_file` helper function to verify the file is ready for use in Bedrock Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_jsonl_file(savepath):\n",
    "    \"\"\"\n",
    "    Tests that a JSON Lines file adheres to the JSONL standard and only contains the \"input\" and \"output\" fields.\n",
    "\n",
    "    Args:\n",
    "        savepath (str): The path to the JSON Lines file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(savepath, \"r\") as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            json_line = json.loads(line)\n",
    "\n",
    "            # Check that the JSON line only contains the \"input\" and \"output\" fields\n",
    "            assert len(json_line.keys()) == 2\n",
    "            assert \"input\" in json_line.keys()\n",
    "            assert \"output\" in json_line.keys()\n",
    "\n",
    "            # Check that the \"input\" and \"output\" fields are not empty\n",
    "            assert json_line[\"input\"] != \"\"\n",
    "            assert json_line[\"output\"] != \"\"\n",
    "\n",
    "            # Check that the JSON line adheres to the JSONL standard (ends with a newline character) as long as it is not the last line\n",
    "            if idx != len(f.readlines()) - 1:\n",
    "                assert line.endswith(\"\\n\")\n",
    "\n",
    "    # Check that the last line of the file does not have a newline character\n",
    "    with open(savepath, \"rb\") as f:\n",
    "        f.seek(-1, os.SEEK_END)\n",
    "        assert f.read() != b\"\\n\"\n",
    "    print(\n",
    "        f'JSON Lines file {savepath} adheres to the JSONL standard and only contains the \"input\" and \"output\" fields.'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's validate our training files are ready for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the train dataset\n",
    "test_jsonl_file(train_path_local)\n",
    "\n",
    "# Test the validation dataset\n",
    "test_jsonl_file(validation_path_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see `JSON Lines file ./output_datasets/train_titan.jsonl adheres to the JSONL standard and only contains the \"input\" and \"output\" fields.` then you can proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Upload training data\n",
    "To use our data for our fine-tuning job on Amazon Bedrock, we need to upload the data sets to an S3 bucket in the same region as the fine-tuning will be happening.\n",
    "\n",
    "The following function uploads the data to S3 for you and returns the S3 URI associated with the uploaded file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s3_uri(bucket_name: str, local_path: str, s3_path: str) -> str:\n",
    "    # Create an S3 client\n",
    "    s3 = boto3.client(\"s3\")\n",
    "\n",
    "    # Upload the file to S3\n",
    "    s3.upload_file(local_path, bucket_name, s3_path)\n",
    "\n",
    "    # Get the S3 URI for the uploaded file\n",
    "    s3_uri = f\"s3://{bucket_name}/{s3_path}\"\n",
    "\n",
    "    return s3_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize our bedrock boto3 client and test to make sure we can successfully call the API.\n",
    "\n",
    "If you receive a 200, you are good to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "bedrock = session.client(\"bedrock\")\n",
    "\n",
    "# Test the connection\n",
    "bedrock.list_foundation_models()['ResponseMetadata']['HTTPStatusCode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = S3_BUCKET_NAME\n",
    "\n",
    "trainingDataS3URI = get_s3_uri(\n",
    "    bucket_name=bucket_name,\n",
    "    local_path=train_path_local,\n",
    "    s3_path=\"data/train_FINAL_20240122.jsonl\"\n",
    ")\n",
    "validationDataS3URI = get_s3_uri(\n",
    "    bucket_name=bucket_name,\n",
    "    local_path=validation_path_local,\n",
    "    s3_path=\"data/dev_FINAL_20240122.jsonl\"\n",
    ")\n",
    "print(trainingDataS3URI)\n",
    "print(validationDataS3URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Kick off a fine-tuning job from the SDK\n",
    "\n",
    "The BOTO3 SDK allows us to kick off a training job or a fine-tuning job on Amazon Bedrock with no manual interaction of the UI.\n",
    "We only need to identify the base model as well as the learning rate, the number of epochs and the other hyper-parameters that can be seen below. You can also follow [our guidelines](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-guidelines.html#model-customization-guidelines-titan-text-express) for fine-tuning Titan Express to fit your use case.\n",
    "\n",
    "For a quick iteration, you might want to set the number of epochs to 1 by setting `epochs = \"1\"`\n",
    "Otherwise, ***as this is currently configured this training job will take up to 12 hours***. If you do decide to change the epochs, you'll want to update your learning rate as well, using the following expression: `newLearningRate = oldLearningRate x newBatchSize / oldBatchSize` per [our documentation for fine-tuning Titan](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-guidelines.html#model-customization-guidelines-titan-text-express).\n",
    "Please go ahead and inspect all of the parameters that have been set in the code below.\n",
    "\n",
    "Notably, you can supply a training and validation data config, which points to your training and validation data sets in S3. You will get an output of loss and perplexity scores at the end of every epoch of your training. These metrics will be stored in a CSV file on S3 for you in the output location that you can follow from the UI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_arn = FINE_TUNING_JOB_ROLE_ARN\n",
    "base_model_identifier = (\n",
    "    f\"arn:aws:bedrock:{bedrock.meta.region_name}::foundation-model/amazon.titan-text-express-v1:0:8k\"\n",
    ")\n",
    "learning_rate = \"1E-6\"\n",
    "epochs = \"10\"\n",
    "\n",
    "## for faster fine-tuning, but poorer results, do the following:\n",
    "# learning_rate = \"1E-6\" \n",
    "# epochs = \"1\" \n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "custom_model_name = f\"ft-spider-{learning_rate}-{epochs}-{current_time}\"\n",
    "jobName = f\"job-{custom_model_name}\"\n",
    "\n",
    "roleArn = (\n",
    "    role_arn\n",
    ")\n",
    "outputDataS3URI = f\"s3://{bucket_name}/titan-output/\" + jobName\n",
    "\n",
    "\n",
    "response = bedrock.create_model_customization_job(\n",
    "    jobName=jobName,\n",
    "    customModelName=custom_model_name,\n",
    "    roleArn=roleArn,\n",
    "    baseModelIdentifier=base_model_identifier,\n",
    "    trainingDataConfig={\"s3Uri\": trainingDataS3URI},\n",
    "    validationDataConfig={\n",
    "        \"validators\": [{\"s3Uri\": validationDataS3URI}]\n",
    "    },\n",
    "    outputDataConfig={\"s3Uri\": outputDataS3URI},\n",
    "    hyperParameters={\n",
    "        \"epochCount\": epochs,\n",
    "        \"batchSize\": \"1\",\n",
    "        \"learningRate\": learning_rate,\n",
    "    }\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand if our model training job has `Completed`, is `In Progress` or has `Stopped`, we can use the BOTO3 SDK once again to inspect the current model customization job.\n",
    "\n",
    "If you train multiple models, just remove the job identifier and filter for the last ones that you would like to inspect.\n",
    "\n",
    "Once your training job has jumped to `Completed`, we are ready to deploy the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if tuning job has finished\n",
    "status = bedrock.get_model_customization_job(jobIdentifier=str(jobName))[\"status\"]\n",
    "\n",
    "if status in (\"Completed\", \"Failed\", \"Stopped\"):\n",
    "    jobStatusFinished = True\n",
    "else:\n",
    "    jobStatusFinished = False\n",
    "\n",
    "status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Create provisioned model\n",
    "In order to support the throughput necessary for running our benchmarks in the next notebook, we need to create a provisioned model. For more information on how Provisioned Models work in Bedrock, [check out our documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/prov-throughput.html) and also note there are [**costs associated with using these**](https://console.aws.amazon.com/bedrock/home#/providers) that vary by model provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provisionedModelName = f\"pvs-{custom_model_name}\"\n",
    "\n",
    "provisioned_model = bedrock.create_provisioned_model_throughput(\n",
    "    modelUnits=1,\n",
    "    provisionedModelName=provisionedModelName,\n",
    "    modelId=custom_model_name,\n",
    ")\n",
    "provisioned_model_arn = provisioned_model['provisionedModelArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure our provisioned model is ready before moving on. Once the Provisioned model is in `InService` status, move on to the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvs_status = bedrock.get_provisioned_model_throughput(\n",
    "    provisionedModelId=provisioned_model_arn\n",
    ")['status']\n",
    "\n",
    "if pvs_status in (\"InService\", \"Failed\"):\n",
    "    ProvisionedModelStatus = True\n",
    "else:\n",
    "    ProvisionedModelStatus = False\n",
    "\n",
    "pvs_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our provisioned throughput model to ensure its working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3_config = Config(read_timeout=1000)\n",
    "bedrock_runtime = session.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    config=boto3_config\n",
    ")\n",
    "\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    modelId=provisioned_model_arn,\n",
    "    contentType=\"application/json\",\n",
    "    accept=\"application/json\",\n",
    "    body=json.dumps(\n",
    "        {\n",
    "            \"inputText\": \"SELECT * FROM table1 WHERE table1.id = 1\",\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "output = response_body.get(\"results\")[0].get(\"outputText\")\n",
    "print(f\"model output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 13: Setup for Benchmarking Model Performance\n",
    "\n",
    "In order to benchmark our fine-tuned models, as well as the models that are available out of the box on Amazon Bedrock. In the following section, you will learn how to invoke the custom models that you fine-tuned as well as the non-fine-tuned models. And we're going to put a specific focus on multi-threading our invocations to Amazon Bedrock endpoints. One of the nice features of having your own provision throughput is that you can expect a very good throughput on your model endpoints. In order to harness this throughput, we really want to go with multi-processing or multi-threading. However, when running a benchmark or utilizing multi-threading or multi-processing, you really need to pay attention on execution order for the `golden-queries` to align with the model generated-queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go any further lets ensure we have our results directory ready to hold our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results directory c\n",
    "results_directory = './results'\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(results_directory):\n",
    "    os.makedirs(results_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build some utility functions to assist with calling Bedrock\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_bedrock(modelId, prompt_data, bedrock_runtime_client, custom_base_model_id=None, system_prompt=\"\"):\n",
    "    \"\"\"\n",
    "    Wraps bedrock.invoke_model to handle input and output parsing.\n",
    "    This method leverages the word-in-mouth technique by starting assistant response with SELECT\n",
    "    :modelId                : the model ID to invoke. Can be custom model id.\n",
    "    :prompt_data            : the text to pass to the model as a prompt. For Anthropic models\n",
    "                            this is used for user message in bedrock messages API\n",
    "    :bedrock_runtime_client : a boto3 bedrock runtime client to use\n",
    "    :custom_base_model_id   : (Optional) custom model flag.\n",
    "                            model id of base model used for customized model.\n",
    "                            indicates use of customized model.\n",
    "    :system_prompt          : (Optional) Used for anthropic models bedrock messages API\n",
    "    \"\"\"\n",
    "    requested_models = f\"{modelId}, {custom_base_model_id}\"\n",
    "    response = None\n",
    "    if \"amazon.\" in requested_models:\n",
    "        body = json.dumps(\n",
    "            {\n",
    "                \"inputText\": f\"{prompt_data} SELECT\",\n",
    "                \"textGenerationConfig\": {\n",
    "                    \"maxTokenCount\": 4096,\n",
    "                    \"stopSequences\": [],\n",
    "                    \"temperature\": 0,\n",
    "                    \"topP\": 0.9,\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "    elif \"anthropic.\" in requested_models:\n",
    "        user_message =  {\"role\": \"user\", \"content\": prompt_data}\n",
    "        asst_message = {\"role\": \"assistant\", \"content\": \"SELECT\"}\n",
    "        messages = [user_message, asst_message]\n",
    "        body=json.dumps(\n",
    "            {\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"max_tokens\": 4096,\n",
    "                \"system\": system_prompt,\n",
    "                \"messages\": messages,\n",
    "                \"temperature\": 0.0,\n",
    "                \"stop_sequences\": [';']\n",
    "            }\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(\"Parameter model must be one of providers amazon or anthropic\")\n",
    "        return response\n",
    "\n",
    "    model_response = bedrock_runtime_client.invoke_model(\n",
    "        body=body,\n",
    "        modelId=modelId,\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "    response_dict = json.loads(model_response.get(\"body\").read().decode(\"utf-8\"))\n",
    "\n",
    "    if \"amazon.\" in requested_models:\n",
    "        response = \"SELECT\" + response_dict[\"results\"][0][\"outputText\"]\n",
    "    elif \"anthropic.\" in requested_models:\n",
    "        response = \"SELECT\" + response_dict[\"content\"][0][\"text\"]\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_retries(model_id, input_data, bedrock_runtime_client, custom_base_model_id=None, max_retries=5, initial_sleep=60):\n",
    "    \"\"\"\n",
    "    calls our bedrock utility function with exponential backoff\n",
    "    :modelId                : the model ID to invoke. Can be custom model id.\n",
    "    :input_data             : the text to pass to the model as a prompt.\n",
    "    :bedrock_runtime_client : a boto3 bedrock runtime client to use\n",
    "    :custom_base_model_id   : (Optional) custom model flag.\n",
    "                            model id of base model used for customized model.\n",
    "                            indicates use of customized model.\n",
    "    :max_retries            : (Optional) maximum number of retries. Defaults to 5.\n",
    "    :initial_sleep          : (Optional) initial sleep time in seconds. Defaults to 60 seconds\n",
    "    \"\"\"\n",
    "    attempts = 0\n",
    "    while attempts < max_retries:\n",
    "        try:\n",
    "            # response = call_bedrock(model_id, input_data[\"input\"])\n",
    "            response = call_bedrock(model_id, input_data, bedrock_runtime_client, custom_base_model_id)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            if \"ModelNotReadyException\" in str(e):\n",
    "                sleep_time = initial_sleep * (2**attempts)\n",
    "                print(f\"Retrying in {sleep_time} seconds...\")\n",
    "                sleep(sleep_time)\n",
    "                attempts += 1\n",
    "            else:\n",
    "                print(f\"Error with input: {input_data['input']}\")\n",
    "                print(e)\n",
    "                return None\n",
    "    return None  # Return None after max_retries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our utility functions on some text to ensure we get a response back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test a call to titan model:\n",
    "modelId = \"amazon.titan-text-express-v1\"\n",
    "call_bedrock(modelId, \"The quick brown fox jumps over the lazy dog\", bedrock_runtime_client=bedrock_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test a call to titan model:\n",
    "modelId = \"amazon.titan-text-express-v1\"\n",
    "call_bedrock(modelId, \"What is Amazon Lambda?\", bedrock_runtime_client=bedrock_runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 14: Run Off-the-shelf Models\n",
    "Here we'll use a few non-fine-tuned models to run the Spider data set questions, that we'll later compare with our fine-tune model for performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function:\n",
    "validation_file_path = (\n",
    "    f\"{output_datasets_dir}/validation_titan.jsonl\"\n",
    ")\n",
    "\n",
    "validation_data = []\n",
    "with open(validation_file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        json_line = json.loads(line)\n",
    "        validation_data.append(json_line)\n",
    "\n",
    "len(validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titan Express\n",
    "Prompt the model for every question in the Spider data set and record the results.\n",
    "Note we're leveraging thread pooling so we can get our results faster through parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"amazon.titan-text-express-v1\"\n",
    "model_name = model_id.split(\".\")[1]\n",
    "# Read the .jsonl file into a list\n",
    "data_list = []\n",
    "with open(validation_file_path, \"r\") as file:\n",
    "    for idx, line in enumerate(file):\n",
    "        json_line = json.loads(line)\n",
    "        data_list.append(json_line)\n",
    "        if idx == 12:\n",
    "            break\n",
    "\n",
    "# Set up ThreadPoolExecutor\n",
    "with ThreadPoolExecutor(10) as executor:\n",
    "    futures = [\n",
    "        executor.submit(call_bedrock, model_id, line['input'], bedrock_runtime) for line in validation_data\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for future in tqdm(futures, total=len(validation_data)):\n",
    "        try:\n",
    "            result = future.result()  # You can add a timeout value here if needed\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(\"Error processing a line.\")\n",
    "            print(e)\n",
    "            results.append(None)  # Append None or some error indicator\n",
    "\n",
    "\n",
    "# Save the results\n",
    "with open(f\"{results_directory}/{model_name}\", \"wb\") as fp:  # Pickling\n",
    "    pickle.dump(results, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Claude 3 Haiku\n",
    "Prompt the model for every question in the Spider data set and record the results.\n",
    "\n",
    "**This process with Claude 3 Haiku can take up to 2 hours.** If you'd like to skip to save time please do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3 Haiku\n",
    "model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "model_name = model_id.split(\".\")[1]\n",
    "\n",
    "answers = []\n",
    "with open(validation_file_path, \"r\") as f:\n",
    "    num_lines = sum(1 for line in f)\n",
    "    f.seek(0)\n",
    "    for idx, line in tqdm(enumerate(f), total=num_lines):\n",
    "        json_line = json.loads(line)\n",
    "        try:\n",
    "            split_string = 'Answer the following question with a SQL Statement:'\n",
    "            user_question = json_line[\"input\"].split(split_string)[1]\n",
    "            system_prompt = json_line[\"input\"].split(split_string)[0] + split_string\n",
    "            response = call_bedrock(\n",
    "                            model_id,\n",
    "                            prompt_data=user_question,\n",
    "                            bedrock_runtime_client=bedrock_runtime,\n",
    "                            system_prompt=system_prompt\n",
    "                        )\n",
    "            answers.append(response)\n",
    "        except Exception as e:\n",
    "            if \"ModelNotReadyException\" in str(e):\n",
    "                print(e)\n",
    "                print(\"waiting 60 seconds and retrying\")\n",
    "                sleep(60)\n",
    "                idx -= 1\n",
    "                continue\n",
    "            else:\n",
    "                print(e)\n",
    "\n",
    "# Save the results\n",
    "with open(f\"{results_directory}/{model_name}\", \"wb\") as fp:  # Pickling\n",
    "    pickle.dump(answers, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Claude Instant\n",
    "Prompt the model for every question in the Spider data set and record the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"anthropic.claude-instant-v1\"\n",
    "model_name = model_id.split(\".\")[1]\n",
    "\n",
    "answers = []\n",
    "with open(validation_file_path, \"r\") as f:\n",
    "    num_lines = sum(1 for line in f)\n",
    "    f.seek(0)\n",
    "    for idx, line in tqdm(enumerate(f), total=num_lines):\n",
    "        json_line = json.loads(line)\n",
    "        try:\n",
    "            split_string = 'Answer the following question with a SQL Statement:'\n",
    "            user_question = json_line[\"input\"].split(split_string)[1]\n",
    "            system_prompt = json_line[\"input\"].split(split_string)[0] + split_string\n",
    "            response = call_bedrock(\n",
    "                            model_id,\n",
    "                            prompt_data=user_question,\n",
    "                            bedrock_runtime_client=bedrock_runtime,\n",
    "                            system_prompt=system_prompt\n",
    "                        )\n",
    "            answers.append(f\"SELECT{response}\")\n",
    "        except Exception as e:\n",
    "            if \"ModelNotReadyException\" in str(e):\n",
    "                print(e)\n",
    "                print(\"waiting 60 seconds and retrying\")\n",
    "                sleep(60)\n",
    "                idx -= 1\n",
    "                continue\n",
    "            else:\n",
    "                print(e)\n",
    "\n",
    "# Save the results\n",
    "with open(f\"{results_directory}/{model_name}\", \"wb\") as fp:  # Pickling\n",
    "    pickle.dump(answers, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 15: Run on our Titan Fine-Tuned Model\n",
    "Here we'll use our fine-tuned model created in the previous notebook to run the same test questions from the Spider data set, just as we've done with the off-the-shelf models. Before doing so, let's make sure our model is responding reasonably to requests. You should see something like \"There are more than 400000 singers in the world.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run all of our validations with our fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up ThreadPoolExecutor\n",
    "with ThreadPoolExecutor(4) as executor:\n",
    "    futures = [\n",
    "        executor.submit(\n",
    "            process_with_retries, \n",
    "            model_id=provisioned_model_arn, \n",
    "            input_data=line[\"input\"], \n",
    "            bedrock_runtime_client=bedrock_runtime,\n",
    "            custom_base_model_id=\"amazon.titan-text-express-v1\")\n",
    "        # print(line)\n",
    "        for line in validation_data\n",
    "    ]\n",
    "    results = []\n",
    "    for future in tqdm(futures, total=len(validation_data)):\n",
    "        try:\n",
    "            result = future.result()  # You can add a timeout value here if needed\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(\"Error processing a line.\")\n",
    "            print(e)\n",
    "            results.append(\"Query could not be completed.\")  # Append string in order to match size of dataframe\n",
    "\n",
    "model_name = custom_model_name\n",
    "\n",
    "# Save the results\n",
    "with open(f\"{results_directory}/{model_name}\", \"wb\") as fp:  # Pickling\n",
    "    pickle.dump(results, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.aws.amazon.com/bedrock/latest/userguide/quotas.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 16: Set Up for Model Performance\n",
    "To analyze our performance across all models, we'll create a pandas dataframe to hold all of our generated queries from the models we're benchmarking. We'll use this analyze results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_bedrock = pd.read_feather(\n",
    "    f\"{output_datasets_dir}/dev_df.feather\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_bedrock.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a couple helper functions to assist.\n",
    "`clean_results` will ensure all of the answers we received from our models are cleansed of leading and trailing whitespace, quotes, and the like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_results(answerlist):\n",
    "    \"\"\"\n",
    "    Cleans a list of strings\n",
    "    :param answerlist: a list of strings\n",
    "    :return: a list of strings\n",
    "    \"\"\"\n",
    "    # Clean the list of strings\n",
    "    clean_list = []\n",
    "    for item in answerlist:\n",
    "        if item:\n",
    "            # Remove any leading or trailing whitespace\n",
    "            item = item.strip()\n",
    "            # Remove any trailing double quotes\n",
    "            if item.startswith('\"') and item.endswith('\"'):\n",
    "                item = item.rstrip('\"')\n",
    "                item = item.lstrip('\"')\n",
    "            # Remove any newlines\n",
    "            item = item.replace(\"\\n\", \" \")\n",
    "            # remove any trailing semi-colons to match validation set\n",
    "            item = item.replace(\";\", \"\")\n",
    "            # Add the cleaned item to the clean_list\n",
    "            clean_list.append(item)\n",
    "\n",
    "    \n",
    "    return clean_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This utility function will compare the results from our model Spider's labeled ground truth to determine the Execution Match accuracy.  This does an exact string comparison of the generated SQL query and gold SQL query, for every question in the Spider data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exact_match_bench(df, model, spider_folder):\n",
    "    \"\"\"\n",
    "    Compares model results to ground truth and returns the accuracy\n",
    "    :param df               : dataframe holding model outputs\n",
    "    :param model            : name of column in data frame results can be found\n",
    "    :param spider_folder    : parent spider directory name that holds the ground truth spider database \n",
    "    :returns                : dict\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    delimiter_char = '|'\n",
    "    counter = 0\n",
    "\n",
    "    for idx in range(0, df.shape[0]):\n",
    "        sql_query = df.iloc[idx][\"query\"]\n",
    "        prediction_query = df.iloc[idx][model]\n",
    "\n",
    "        db_id = df.iloc[idx][\"db_id\"]\n",
    "        db_file = f\"{spider_folder}/database/{db_id}/{db_id}.sqlite\"\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        cursor = conn.cursor()\n",
    "        result_row = {}\n",
    "        try:\n",
    "            # Fetching the gold standard\n",
    "            cursor.execute(sql_query)\n",
    "            result_gold = cursor.fetchall()\n",
    "            result_row['gold_query'] = sql_query\n",
    "            result_row['gold_result'] = result_gold\n",
    "            \n",
    "            try:\n",
    "                # Fetching prediction results\n",
    "                cursor.execute(prediction_query)\n",
    "                result_preds = cursor.fetchall()\n",
    "                result_row['prediction_query'] = prediction_query\n",
    "                result_row['prediction'] = result_preds\n",
    "            except Exception as e:\n",
    "                result_row['prediction_query'] = prediction_query\n",
    "                result_row['prediction'] = result_preds\n",
    "                continue\n",
    "\n",
    "            # Comparing the results\n",
    "            if result_gold == result_preds:\n",
    "                result_row['match'] = True\n",
    "                counter += 1\n",
    "            else:\n",
    "                result_row['match'] = False\n",
    "        except Exception as e:\n",
    "            error = \"General error\\n\"\n",
    "            result_row['match'] = False\n",
    "            result_row['error'] = e\n",
    "        results.append(result_row)\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(\n",
    "        path_or_buf=f\"comparison_{model}.txt\", \n",
    "        sep=',', \n",
    "        header=True, \n",
    "        index=False, \n",
    "        mode='w', \n",
    "    )\n",
    "\n",
    "    print(f\"{model} Accuracy: {counter/df.shape[0]}\")\n",
    "    return {counter / df.shape[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 17: Evaluate Models\n",
    "With our utility functions available, we'll iterate through each of our output results files, aggregate the accuracy as measured against the labeled Spider data set, and display the accuracy of each model. While we're at it, we'll also add the generated SQL to our `df_eval_bedrock` data frame for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add results from all tests to benchmark dataframe\n",
    "# build accuracy results\n",
    "accuracy_results = []\n",
    "directory = os.fsencode(results_directory)\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename not in ('.ipynb_checkpoints'):\n",
    "        dir = os.fsdecode(directory)\n",
    "        with open(f\"{dir}/{filename}\", \"rb\") as fp:  # Unpickling\n",
    "            results = pickle.load(fp)\n",
    "            cleaned_results = clean_results(results)\n",
    "            try:\n",
    "                df_eval_bedrock[filename] = cleaned_results\n",
    "                accuracy = run_exact_match_bench(df_eval_bedrock, filename, spider_folder)\n",
    "                accuracy_results.append(accuracy)\n",
    "            except:\n",
    "                print(f\"couldn't add for {dir}/{filename}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 18: Analyze Results\n",
    "Now that we've benchmarked our models, including our fine-tuned model, take a look at our dataframe to compare how the models generated SQL.\n",
    "\n",
    "How much of an improvement over the basemodel Titan-Express did our Fine-Tuned Model do?\n",
    "Why might some of Claude's code not have matched? Did it alias columns? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_bedrock.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see why some claude responses may not have matched by loading our fine-tuned results into a pandas dataframe and filtering to non-matches.\n",
    "\n",
    "How would you improve your prompting to better account for these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fine_tuned_result=pd.read_csv(f\"comparison_{custom_model_name}.txt\", sep=',', header=1, names=['gold_query','gold_result','prediction_query','prediction','match','error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fine_tuned_result.loc[df_fine_tuned_result['match'] == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this notebook we used the Spider data set to fine-tune a Titan Text Model to be optimized for Text-to-SQL.\n",
    "\n",
    "Its important to remember that our fine-tuned model is now optimized to accomplish a much narrower scope of tasks: convert a natural language question to a SQL query.\n",
    "Specifically, we've fine-tuned on the Spider data set so it will use its understanding of this data set when responding to questions. This means using this model on a data set other than Spider will yield poor results. \n",
    "Secondly, the SQL used in the Spider data set is biased for SQL-lite syntax. This will add to poor performance when used against databases other than SQL-Lite.\n",
    "\n",
    "Those wishing to fine-tune a model on their own data will require a fully labeled list of questions and SQL queries, just like Spider. This effort can consitute a significant engineering cost to the project, as a human must carefully curate questions with their syntactically correct SQL query."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
