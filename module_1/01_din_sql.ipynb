{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Prompting for Text-to-SQL: DIN-SQL\n",
    "Use of advanced prompting techniques to convert a natural language question to SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested SageMaker Environment\n",
    "Sagemaker Image: sagemaker-distribution-cpu\n",
    "\n",
    "Kernel: Python 3\n",
    "\n",
    "Instance Type: ml.m5.large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Install Dependencies](#step-1-install-dependencies)\n",
    "1. [Set up Athena Connection](#step-2-set-up-connection-to-the-tpc-ds-data-set-in-athena)\n",
    "1. [Schema Linking](#step-3-determine-schema-links)\n",
    "1. [Classify Query Complexity](#step-4-classify-sql-complexity)\n",
    "1. [Generate SQL Query](#step-5-generate-sql-query)\n",
    "1. [Execute SQL Query](#step-6-execute-query)\n",
    "1. [Validate Results](#step-7-validate-results)\n",
    "1. [Self-Correction](#step-8-self-correction)\n",
    "1. [Experiment](#step-9-experiment)\n",
    "1. [Citation](#citation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "This notebook will provide code snippets to assist with implementing one approach to converting a natural language question into a SQL query that would answer it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Approach to the Text-to-SQL Problem\n",
    "We'll implement the DIN-SQL prompting strategy to break a question down into smaller parts, get an understanding of the query complexity, and ultimately create a valid SQL statement. As shown below, this process consists of four main prompting steps:\n",
    "\n",
    "1. Schema Linking\n",
    "2. Classification and decomposition\n",
    "3. SQL code generation\n",
    "4. Self-correction\n",
    "\n",
    "For a deeper dive into the methodology and findings about this approach, please read the full paper here: https://arxiv.org/pdf/2304.11015.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](content/din_sql_methodology.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools\n",
    "SQLAlchemy, Anthropic, Amazon Bedrock SDK (Boto3), PyAthena, Jinja2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will install all the required dependencies to run this notebook. **You can ignore the following errors** that may arise due to dependency conflicts for libraries we won't be using in this module:\n",
    "```\n",
    "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
    "dash 2.14.1 requires dash-core-components==2.0.0, which is not installed.\n",
    "dash 2.14.1 requires dash-html-components==2.0.0, which is not installed.\n",
    "dash 2.14.1 requires dash-table==5.0.0, which is not installed.\n",
    "jupyter-ai 2.5.0 requires faiss-cpu, which is not installed.\n",
    "amazon-sagemaker-jupyter-scheduler 3.0.4 requires pydantic==1.*, but you have pydantic 2.6.0 which is incompatible.\n",
    "gluonts 0.13.7 requires pydantic~=1.7, but you have pydantic 2.6.0 which is incompatible.\n",
    "jupyter-ai 2.5.0 requires pydantic~=1.0, but you have pydantic 2.6.0 which is incompatible.\n",
    "jupyter-ai-magics 2.5.0 requires pydantic~=1.0, but you have pydantic 2.6.0 which is incompatible.\n",
    "jupyter-scheduler 2.3.0 requires pydantic~=1.10, but you have pydantic 2.6.0 which is incompatible.\n",
    "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.1.2 which is incompatible.\n",
    "tensorflow 2.12.1 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.9.0 which is incompatible.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m ensurepip --upgrade\n",
    "!pip install \"sqlalchemy\" --quiet\n",
    "!pip install \"boto3~=1.34\"  --quiet\n",
    "!pip install \"jinja2\" --quiet\n",
    "!pip install \"botocore\" --quiet\n",
    "!pip install \"pandas\" --quiet\n",
    "!pip install \"PyAthena\" --quiet\n",
    "!pip install \"faiss-cpu\" --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the `din_sql` library to assist with using the prompts written in the paper. Note that we've leveraged Jinja for our prompt templating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "from libs.din_sql import din_sql_lib as dsl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set up connection to the TPC-DS data set in Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the following variables with details relative to your account, and how you setup the Athena data source connector for the TPC-DS dataset. You can find in these in the CloudFormation outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATHENA_RESULTS_S3_LOCATION = \"<workshop bucket name>\" # available in cloudformation outputs\n",
    "ATHENA_CATALOG_NAME = \"<athena catalog name>\" # available in cloudformation outputs\n",
    "DB_NAME = \"tpcds1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instantiate the `din_sql` class with the bedrock model of your choice. In this module, the prompts are tailored specifically to work well with ClaudeV2, so we'll be using that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "din_sql = dsl.DIN_SQL(bedrock_model_id='anthropic.claude-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a connection to Athena using the information entered above. We'll use this connection to test our generated SQL. Its also used to augment prompts in DIN-SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "din_sql.athena_connect(catalog_name=ATHENA_CATALOG_NAME, \n",
    "               db_name=DB_NAME, \n",
    "               s3_prefix=ATHENA_RESULTS_S3_LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Determine Schema Links "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in the DIN-SQL process is to find out which foreign key relationships are required in order to answer the question. Let's take a look at how the prompt for this task is designed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ../libs/din_sql/prompt_templates/schema_linking_prompt.txt.jinja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_sql= din_sql.find_fields(db_name=DB_NAME)\n",
    "print(return_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take a look at the prompt template, you can see we're using some [Anthropic Prompting best practices](https://docs.anthropic.com/claude/docs/introduction-to-prompt-design) to improve results when working with Claude:\n",
    "1. [Mark different parts of the prompt](https://docs.anthropic.com/claude/docs/constructing-a-prompt#mark-different-parts-of-the-prompt) using XML tags. In our example, we use xml tags and ```sql to organize our output\n",
    "2. [We use many examples](https://docs.anthropic.com/claude/docs/constructing-a-prompt#examples-optional) This prompt technique uses a many-shot method by offering Claude a lot of examples.\n",
    "3. [We ask Claude to think step-by-step](https://docs.anthropic.com/claude/docs/ask-claude-to-think-step-by-step)\n",
    "4. We use [Roleplay Dialogue](https://docs.anthropic.com/claude/docs/roleplay-dialogue) to help Claude act the part of a relational database expert.\n",
    "\n",
    "Lets see how our prompt will look by passing in a question and database name to the `schema_linking_prompt_maker` method. Note the use of tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Which customer spent the most money in the web store?\"\n",
    "\n",
    "schema_links_prompt = din_sql.schema_linking_prompt_maker(question, DB_NAME)\n",
    "print(schema_links_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our schema link prompt, lets see what Claude comes up with for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_links = din_sql.llm_generation(\n",
    "                    schema_links_prompt,\n",
    "                    stop_sequences=['</example>']\n",
    "                    )\n",
    "print(schema_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Claude reasoned its way through identifying the foreign key relationships between tables. This is because we gave it a list of tables and their columns for Claude to inspect. Lets use those `<link>` tags to clean up our response, and store this list for our next step in the DIN-SQL method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = schema_links.split('<links>')[1].split('</links>')[0].replace('\\n','')\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Classify SQL Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in the process is to classify the complexity of the SQL that will be required to answer the question. Lets take a look at the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ../libs/din_sql/prompt_templates/classification_prompt.txt.jinja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're giving Claude a decision making framework for determining if the class of the query required to answer the question. This is done by offering simple if/then logic.\n",
    "\n",
    "Feel free to take a closer look at how this prompt uses examples of each class to teach Claude how to make decisions. Once complete, go ahead and send your prompt to Claude to classify the complexity of this query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = din_sql.llm_generation(\n",
    "    din_sql.classification_prompt_maker(question, DB_NAME, links)\n",
    "    )\n",
    "print(classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that Claude is taking advantage of the room we gave it think about the decision. Let's parse the result using the `<label>` tag and move onto SQL code generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class = classification.split(\"<label>\")[1].split(\"</label>\")[0].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Generate SQL Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our question in hand, complexity of the requisite query classified, and our schema links identified, we are now ready to generate our SQL statement. Before we do that, lets look at the prompt. Since 'NON-NESTED' classes use the 'medium_prompt' template, we'll take a look at that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ../libs/din_sql/prompt_templates/medium_prompt.txt.jinja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because these types of SQL queries require a join, these prompts give many examples using a join so Claude understands how to use one. Let's send our prompt to Claude to see what it generates. Note we are levaraging our example end tag, that we used in our prompt, to act as our stop sequence so Claude will stop generating a response if its following the format we've instructed it to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_qry = din_sql.llm_generation(\n",
    "                    prompt=din_sql.medium_prompt_maker(\n",
    "                        test_sample_text=question, \n",
    "                        database=DB_NAME, \n",
    "                        schema_links=links,\n",
    "                        sql_tag_start='```sql',\n",
    "                        sql_tag_end='```'),\n",
    "                    stop_sequences=['</example>'])\n",
    "print(f\"{sql_qry}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see now how Claude is following the instructions, thinking step-by-step, and enclosing the SQL statement in our chosen tags. Lets parse out the last query, as it will always be the last in the chain of thought process that should be the most accurate, per our instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL = sql_qry.split('```sql')[-1].split('```')[0]\n",
    "print(f\"{SQL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Execute Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our query to see if the results match what we would expect to see, and if it actually answers our question. We'll do this by returning our SQL Alchemy result set and using a Pandas Data Frame to interact with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result_set = din_sql.query(SQL)\n",
    "pd.DataFrame(result_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Validate Results\n",
    "Let's make sure this answer is correct by submitting a query we know will list the top 10 customers by web sales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_query = \"\"\"\n",
    "    SELECT \"c\".\"c_customer_sk\"\n",
    "    , \"c\".\"c_first_name\"\n",
    "    , \"c\".\"c_last_name\"\n",
    "    , SUM(\"ws\".\"ws_ext_list_price\") as total_sales\n",
    "    FROM \"customer\" \"c\" \n",
    "    JOIN \"web_sales\" \"ws\" \n",
    "        ON \"ws\".\"ws_bill_customer_sk\" = \"c\".\"c_customer_sk\"   \n",
    "    GROUP BY \"c\".\"c_customer_sk\"\n",
    "    , \"c\".\"c_first_name\"\n",
    "    , \"c\".\"c_last_name\"\n",
    "    ORDER BY total_sales desc\n",
    "    limit 10\n",
    "\"\"\"\n",
    "validation_set = din_sql.query(validation_query)\n",
    "pd.DataFrame(validation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see the same Customer SK at the top of the list? Which fields did the generated query use, versus the corret one?\n",
    "If the query threw an error, feel free to move onto Self Correction where we'll let the LLM correct the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Self Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the last step in the process. We make one last check of our SQL code by asking Claude to fix anything that is wrong with the code for the given SQL dialect. Let's take a look at those instructions now to see how that is done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ../libs/din_sql/prompt_templates/clean_query_prompt.txt.jinja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use this template to create a prompt for our query using the \"presto\" syntax, which is what Athena uses for querying its underlying data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revised_sql = din_sql.debugger_generation(\n",
    "            prompt=din_sql.debugger(question, DB_NAME, SQL, sql_dialect='presto')\n",
    "            ).replace(\"\\n\", \" \")\n",
    "print(f\"{revised_sql}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our revised SQL returned, lets parse it out of the response using our code tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL = revised_sql.split('```sql')[1].split('```')[0].strip()\n",
    "print(f\"{SQL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the results what you expected? If not, how can you improve the prompting to generalize better?\n",
    "\n",
    "Below is another run through of the process end-to-end for a different question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'What year had the highest catalog sales?'\n",
    "\n",
    "#get schema links\n",
    "schema_links_prompt = din_sql.schema_linking_prompt_maker(question, DB_NAME)\n",
    "schema_links = din_sql.llm_generation(\n",
    "                    schema_links_prompt\n",
    "                    )\n",
    "print(schema_links)\n",
    "links = schema_links.split('<links>')[1].split('</links>')[0].replace('\\n','')\n",
    "\n",
    "# classify and decompose\n",
    "classification = din_sql.llm_generation(\n",
    "    din_sql.classification_prompt_maker(question, DB_NAME, links)\n",
    "    )\n",
    "print(classification)\n",
    "predicted_class = classification.split(\"<label>\")[1].split(\"</label>\")[0].strip()\n",
    "\n",
    "# generate SQL\n",
    "sql_qry = din_sql.llm_generation(\n",
    "                    prompt=din_sql.medium_prompt_maker(\n",
    "                        test_sample_text=question, \n",
    "                        database=DB_NAME, \n",
    "                        schema_links=links,\n",
    "                        sql_tag_start='```sql',\n",
    "                        sql_tag_end='```'),\n",
    "                    stop_sequences=['</example>'])\n",
    "print(f\"{sql_qry}\")\n",
    "SQL = revised_sql.split('```sql')[1].split('```')[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(din_sql.query(SQL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the query threw an error, try self-correction once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#self correction\n",
    "revised_sql = din_sql.debugger_generation(\n",
    "            prompt=din_sql.debugger(question, DB_NAME, SQL, sql_dialect='presto')\n",
    "            ).replace(\"\\n\", \" \")\n",
    "print(f\"{revised_sql}\")\n",
    "SQL = revised_sql.split('```sql')[1].split('```')[0].strip()\n",
    "print(f\"{SQL}\")\n",
    "\n",
    "# see results\n",
    "result_set = pd.DataFrame(din_sql.query(SQL))\n",
    "result_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citation\n",
    "```\n",
    "@article{pourreza2023din,\n",
    "  title={DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction},\n",
    "  author={Pourreza, Mohammadreza and Rafiei, Davood},\n",
    "  journal={arXiv preprint arXiv:2304.11015},\n",
    "  year={2023}\n",
    "}\n",
    "Paper: https://arxiv.org/abs/2304.11015\n",
    "Code: https://github.com/MohammadrezaPourreza/Few-shot-NL2SQL-with-prompting\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfl_rag_project-k-HlTiHq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
