{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0916a3a-e402-48b7-a775-ce739e4aeaf4",
   "metadata": {},
   "source": [
    "# Text-to-SQL Using Retrieval Augmented Generation (RAG)\n",
    "Use of RAG to improve performance of Text-to-SQL use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c217c20",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ee7e7e",
   "metadata": {},
   "source": [
    "## Suggested SageMaker Environment\n",
    "Sagemaker Image: sagemaker-distribution-cpu\n",
    "\n",
    "Kernel: Python 3\n",
    "\n",
    "Instance Type: ml.m5.large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bafc2f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612f32bc",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Install Dependencies](#step-1-install-dependencies)\n",
    "1. [Configure Bedrock Embeddings](#step-2-configure-bedrock-embedding-model-and-llm)\n",
    "1. [Configure Athena and Bedrock Client](#step-3-configure-athena-and-bedrock-client)\n",
    "1. [Create Helper Functions](#step-4-create-helper-functions)\n",
    "1. [Configure Bedrock Embedding Model](#step-5-configure-bedrock-embedding-model)\n",
    "1. [Fetch TPD-DS Meta Data](#step-6-fetch-tpc-ds-dataset-tables-and-columns-information)\n",
    "1. [Embed Questions and Metadata](#step-7-embed-all-the-questions-and-metadata)\n",
    "1. [Build Prompt and Generate Query](#step-8-build-prompt-and-generate-sql-query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f2bd54",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171260d1",
   "metadata": {},
   "source": [
    "## Objective\n",
    "This notebook will provide code snippets to assist with implementing one approach to converting a natural language question into a SQL query that would answer it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6908e9c8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7357e555",
   "metadata": {},
   "source": [
    "## The Approach to the Text-to-SQL Problem\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "770bec8e-dd15-4b7d-8ec3-bc35baab0305",
   "metadata": {},
   "source": [
    "We'll walk through seting up a Bedrock embedding Model and LLM to Embed the table metadata.\n",
    "\n",
    "First, we'll get the metadata from Athena.\n",
    "\n",
    "Second, we'll use the meta data and ask the LLM to generate possible questions that could be answered with each table. \n",
    "\n",
    "Third, we'll embed all the metadata and generated questions in a Vector Store. We'll use an in-memory Vector store called [FAISS](https://faiss.ai/index.html), but one could also use a long-running store such as Amazon OpenSearch. We'll use semantic similarity to retrieve tables and columns that could help us answer the question being asked.\n",
    "\n",
    "Last, We'll design a robust prompt to incorporate our embeddings, instructions, few-shot examples, and of course our question."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c81e0a1c",
   "metadata": {},
   "source": [
    "![Alt text](content/rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50273671",
   "metadata": {},
   "source": [
    "### Tools\n",
    "Langchain, Amazon Bedrock SDK (boto3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70a7762",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b00409e",
   "metadata": {},
   "source": [
    "### Step 1: Install Dependencies\n",
    "\n",
    "Here we will install all the required dependencies to run this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455ba14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m ensurepip --upgrade\n",
    "!pip install \"sqlalchemy\" --quiet\n",
    "!pip install \"boto3~=1.34\"  --quiet\n",
    "!pip install \"jinja2\" --quiet\n",
    "!pip install \"botocore\" --quiet\n",
    "!pip install \"pandas\" --quiet\n",
    "!pip install \"PyAthena\" --quiet\n",
    "!pip install \"faiss-cpu\" --quiet\n",
    "!pip install langchain --quiet\n",
    "!pip install jq --quiet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c1fda97-9150-484a-8cfa-86ec9568fc61",
   "metadata": {},
   "source": [
    "### Step 2: Configure Bedrock embedding Model and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c70660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from langchain.document_loaders.json_loader import JSONLoader\n",
    "from langchain.docstore.document import Document\n",
    "import json\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "import re\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from functools import reduce\n",
    "from langchain.prompts import PromptTemplate\n",
    "from sqlalchemy import MetaData\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "sys.path.append('../')\n",
    "from libs.din_sql import din_sql_lib as dsl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07958746-7313-484a-887b-167b8d8acf31",
   "metadata": {},
   "source": [
    "### Step 3: Configure Athena and Bedrock Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2212b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATHENA_RESULTS_S3_LOCATION = \"<workshop bucket name>\" # available in cloudformation outputs\n",
    "ATHENA_CATALOG_NAME = \"<athena catalog name>\" # available in cloudformation outputs\n",
    "DB_NAME = \"tpcds1\"\n",
    "DB_FAISS_PATH = './vectorstore/db_faiss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcd2cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_region = athena_region = boto3.session.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18230081",
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_config = Config(retries = {'max_attempts': 100})\n",
    "session = boto3.Session(region_name=bedrock_region)\n",
    "bedrock = session.client('bedrock-runtime', region_name=bedrock_region, config=retry_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce22c308-ebbf-4ef5-a823-832b7c236e31",
   "metadata": {},
   "source": [
    "### Step 4: Create Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0777db36",
   "metadata": {},
   "source": [
    "Here we wrap our bedrock call into a method to accept a question and a model id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bf3acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm(question,modelId):\n",
    "    \n",
    "    body = ''\n",
    "    if 'titan' in modelId:\n",
    "         model_kwargs = {\"maxTokenCount\": 200, \"temperature\": 0.001} \n",
    "         input_body = dict()\n",
    "         input_body[\"inputText\"] = question\n",
    "         input_body[\"textGenerationConfig\"] = {**model_kwargs}\n",
    "         body = json.dumps(input_body)\n",
    "\n",
    "    else:\n",
    "          body = json.dumps({\n",
    "                    \"prompt\": question,\n",
    "                    \"max_tokens_to_sample\":4096,\n",
    "                    \"temperature\":0.5,\n",
    "                    \"top_k\":250,\n",
    "                    \"top_p\":0.5,\n",
    "                  }) \n",
    "    \n",
    "    accept = 'application/json'\n",
    "    contentType = 'application/json'\n",
    "\n",
    "    response = bedrock.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    print(response_body)\n",
    "    return response_body"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24f7187d",
   "metadata": {},
   "source": [
    "Once the LLM returns the list of questions that can be answered using the given table and column, our `write_questions_to_file` method will handle storing them locally in a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e571d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_questions_to_file(question_list_filename, table_name, table_schema, answer):\n",
    "    data_list = []\n",
    "    question_list_obj = answer\n",
    "    questions_list = question_list_obj.splitlines()\n",
    "    print(questions_list)\n",
    "    # Open the file in write mode\n",
    "    with open(question_list_filename, mode=\"w\", newline=\"\") as file:\n",
    "        for question in questions_list:\n",
    "\n",
    "            # Skip if it doesn't really have a question\n",
    "            if \"?\" not in question:\n",
    "                continue\n",
    "\n",
    "            questionSplit = re.split(r\"\\d{1,5}.||. ||- \", question, maxsplit=1)\n",
    "            print(questionSplit)\n",
    "            question = questionSplit[1]\n",
    "            data = {\n",
    "                \"tableName\": table_name,\n",
    "                \"question\": question,\n",
    "                \"tableSchema\": table_schema.lstrip(\" \"),\n",
    "            }\n",
    "            data_list.append(data)\n",
    "\n",
    "        json.dump(data_list, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a0c538",
   "metadata": {},
   "source": [
    "We'll need a method to accept a list of documents, and return a list of the same documents with their metadata attached. We'll also need a function to help us load a json and return a JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad760cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new docs with the right metadata we need for indexing\n",
    "def create_docs_with_correct_metadata(documents):\n",
    "    # We are going to return a list of new documents\n",
    "    new_docs = []\n",
    "\n",
    "    # For each document\n",
    "    for doc in documents:\n",
    "        # Get it's metadata and contents\n",
    "        metadata = doc.metadata\n",
    "        contents = json.loads(doc.page_content)\n",
    "\n",
    "        # Now calculate the new metadata that we want to add\n",
    "        new_metadata = {\n",
    "            \"tableName\": contents[\"tableName\"],\n",
    "            \"question\": contents[\"question\"],\n",
    "            \"tableSchema\": contents[\"tableSchema\"],\n",
    "        }\n",
    "\n",
    "        # Print out the new metadata for our documents\n",
    "        # print(new_metadata)\n",
    "\n",
    "        new_docs.append(\n",
    "            Document(page_content=new_metadata[\"question\"], metadata=new_metadata)\n",
    "        )\n",
    "\n",
    "    return new_docs\n",
    "\n",
    "def load_json_file(filename):\n",
    "    loader = JSONLoader(file_path=filename, jq_schema=\".[]\", text_content=False)\n",
    "\n",
    "    # This is our internal Langchain document data structure\n",
    "    docs = loader.load()\n",
    "    return docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a1d76a9",
   "metadata": {},
   "source": [
    "This function asks the LLM to inspect a table schema, generate some questions which could be answered by that schema, and then it stores those questions to a file. Finally, it appends them all into a single vector database.\n",
    "The below prompt is used to get the questions in natural language and it calls all the helper function to embed the questions and table metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4367048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function asks the LLM to inspect a table schema, generate some questions which could be answered\n",
    "# by that schema, and then it stores those questions to file, loads them all into a single vectorDB\n",
    "def add_new_table(schema, table_name,model_id,is_incremental, bedrock_embeddings):\n",
    "    \"\"\"\n",
    "    :schema         :   \n",
    "    :table_name     :\n",
    "    :model_id       :\n",
    "    :is_incremental :\n",
    "    \"\"\"\n",
    "    print(f\"Adding table {table_name} with schema {schema}\")\n",
    "    \n",
    "    question = f\"\"\"\n",
    "    \\n\\nHuman: \n",
    "    only return the a bulleted numbered list of unique and detailed questions that could be answered by this table called {table_name} with schema:\n",
    "    {schema}.\n",
    "    Instructions:\n",
    "        Use natural language descriptions only.\n",
    "        Do not use SQL.\n",
    "        Produced a varied list of questions, but the questions should be unique and detailed.\n",
    "        The questions should be in a format that is easy to understand and answer.\n",
    "        Ask about as much of the information in the table as possible.\n",
    "        You can ask about more than one aspect of the data at a time.\n",
    "        Qustions should begin with, 'What', 'Which', 'How', 'When' or 'Can'. Use variable names. \n",
    "        The questions should use relevant buisness vocabularly and terminology only. \n",
    "        Do not use column names in your output - use relevant natural language descriptions only. \n",
    "        Do not output any numeric values.\n",
    "        Output questions starting with bulleted numbered list. \n",
    "         \n",
    "        \\n Questions: 1.\n",
    "        \\n Assistant:\n",
    "        \"\"\"\n",
    "       \n",
    "\n",
    "    response = ask_llm(question,model_id)\n",
    "    answer = response['completion']\n",
    "    question_list_filename = f\"../questionList{table_name}.json\"\n",
    "\n",
    "    # # Get rid of anything before the 1.\n",
    "    # if re.match(r\"^[^\\d+]\\. \", answer) and re.search(r\"\\d+\\. \", answer):\n",
    "    #     answer = \"1. \" + answer.split(\"1. \")[1]\n",
    "    # else:\n",
    "    #     answer = \"1. \" + answer\n",
    "\n",
    "    print(\n",
    "        f\"Writing questions to {question_list_filename}, with schema {schema}, with table name {table_name} and answer {answer}.\\n\\n\"\n",
    "    )\n",
    "\n",
    "    write_questions_to_file(question_list_filename, table_name, schema, answer)\n",
    "    docs = load_json_file(question_list_filename)\n",
    "    docs = create_docs_with_correct_metadata(docs)\n",
    "    new_questions = FAISS.from_documents(docs, bedrock_embeddings)\n",
    "    db_exists = True if os.path.exists(f\"{DB_FAISS_PATH}/index.faiss\") else False\n",
    "    # Add new tables\n",
    "    if is_incremental and db_exists:\n",
    "            question_db = FAISS.load_local(DB_FAISS_PATH, bedrock_embeddings, allow_dangerous_deserialization=True)\n",
    "            question_db.merge_from(new_questions)\n",
    "            question_db.save_local(DB_FAISS_PATH)\n",
    "\n",
    "    # Load for the first time\n",
    "    else:\n",
    "        print(f\"is_incremental set to {str(is_incremental)} and/or no vector db found. Creating...\")\n",
    "        new_questions.save_local(DB_FAISS_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a486f1a5",
   "metadata": {},
   "source": [
    " ### Step 5: Configure Bedrock Embedding Model\n",
    " Here we'll create a LangChain Embedding model to be used for converting text to vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb53761",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_embeddings = BedrockEmbeddings(client=bedrock)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49fc5e86",
   "metadata": {},
   "source": [
    " ### Step 6: Fetch TPC-DS Dataset Tables and Columns Information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f4afd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sqlalchemy_athena(database,catalog,s3stagingathena,region):\n",
    "\n",
    "    athena_connection_str = f'awsathena+rest://:@athena.{region}.amazonaws.com:443/{database}?s3_staging_dir={s3stagingathena}&catalog_name={catalog}'\n",
    "    # Create Athena engine\n",
    "    athena_engine = create_engine(athena_connection_str) \n",
    "    return athena_engine\n",
    "\n",
    "\n",
    "def get_tpc_ds_dataset(database,catalog,s3stagingathena,region):\n",
    "# Reflect db schema\n",
    "    \n",
    "    column_table  =[]\n",
    "    columns_str= ''\n",
    "    table_name = ''\n",
    "    metadata = MetaData()\n",
    "    engine = get_sqlalchemy_athena(database,catalog,s3stagingathena,region)\n",
    "    metadata.reflect(bind=engine)\n",
    "\n",
    "    # Get list of table names\n",
    "    print(metadata.tables.keys()) \n",
    "\n",
    "    # Loop through tables\n",
    "    for table in metadata.tables:\n",
    "        print(f\"Table: {table}\")\n",
    "        table_name= table\n",
    "        columns_str= ''\n",
    "        tuple = ''\n",
    "        print(f\"Schema: {metadata.tables[table].schema}\")\n",
    "        print(f\"Columns: {metadata.tables[table].columns.keys()}\")                \n",
    "\n",
    "        for column in metadata.tables[table].columns.keys():\n",
    "                columns_str=columns_str+f'{column}'+\"|\"  \n",
    "                \n",
    "        tuple = columns_str,table_name\n",
    "        column_table.append(tuple)      \n",
    "    return  column_table      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46f4c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpc_ds = get_tpc_ds_dataset(DB_NAME,ATHENA_CATALOG_NAME,ATHENA_RESULTS_S3_LOCATION,athena_region)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbb3a532",
   "metadata": {},
   "source": [
    "### Step 7: Embed all the questions and metadata \n",
    "Here we'll use our helper functions to embed table meta data and generate plausible questions we could ask of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2cdf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'anthropic.claude-v2'\n",
    "\n",
    "for x in tpc_ds:\n",
    "    print(x)\n",
    "    add_new_table(\n",
    "        schema=x[0], \n",
    "        table_name=x[1],\n",
    "        model_id=model_id,\n",
    "        is_incremental=True, \n",
    "        bedrock_embeddings=bedrock_embeddings\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0ff71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_db = FAISS.load_local(DB_FAISS_PATH, bedrock_embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d87ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Find the top 10 customer name by total dollars spent\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada34d48",
   "metadata": {},
   "source": [
    "### Step 8: Build Prompt and Generate SQL Query\n",
    "First we'll get the tables and column information using both similarity and Keyword search to pull in possible matches based on the semantic meaning of our question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454d0043",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema =  {}\n",
    "\n",
    "results_with_scores = question_db.similarity_search_with_score(query)\n",
    "for doc, score in results_with_scores:\n",
    "    print(doc.metadata['question'])\n",
    "    schema[doc.metadata['tableName']] = doc.metadata['tableSchema']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b011b7",
   "metadata": {},
   "source": [
    "Initialize our DIN_SQL class with the anthropic claude v2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf5cc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "din_sql = dsl.DIN_SQL(bedrock_model_id=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21675ed4",
   "metadata": {},
   "source": [
    "Connect to Athena to prepare for executing a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a142a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "din_sql.athena_connect(catalog_name=ATHENA_CATALOG_NAME, \n",
    "               db_name=DB_NAME, \n",
    "               s3_prefix=ATHENA_RESULTS_S3_LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2692f8",
   "metadata": {},
   "source": [
    "Now we'll augment our `schema` object with any table metadata that match the table name to any word in our question, to catch any obvious matches not accounted for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac1432",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tables = din_sql.find_tables(DB_NAME)\n",
    "list_words = query.split(\" \")\n",
    "\n",
    "intersection = reduce(lambda acc, x: acc + [x] if x in list_words and x not in acc else acc, list_tables, [])\n",
    "for table in  intersection :\n",
    "   if table in schema:\n",
    "      print(\"exists\")\n",
    "   else:\n",
    "      schema_name = din_sql.get_schema(DB_NAME,table)\n",
    "      schema[table] = schema_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ae5e61",
   "metadata": {},
   "source": [
    "Let's take a look at what's in our `schema` object now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0ab256",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb64e31",
   "metadata": {},
   "source": [
    "With our schema information ready for use, we're now ready for a prompt that can get use quality results. Take a look at the following prompt that use Claude Prompting best practices, and see how our schema is incorporated into the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c55bf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\\n\\nHuman: \n",
    "        <Instructions>\n",
    "            Read database schema inside the <database_schema></database_schema> tags which contains a json list of table names and their pipe-delimited schemas to do the following:\n",
    "            1. Create a syntactically correct awsathena query to answer the question.\n",
    "            2. Never query for all the columns from a specific table, only ask for a few relevant columns given the question.\n",
    "            3. Pay attention to use only the column names that you can see in the schema description. \n",
    "            4. Be careful to not query for columns that do not exist. \n",
    "            5. Pay attention to which column is in which table. \n",
    "            6. Qualify column names with the table name when needed. You are required to use the following format, each taking one line:\n",
    "            7. Return the sql query inside the <SQL></SQL> tab.\n",
    "        </Instructions>\n",
    "          \n",
    "        <database_schema>{schema}</database_schema>\n",
    "        \n",
    "        <examples>\n",
    "        <Question>\"How many users do we have?\"</Question>\n",
    "        <SQL>SELECT SUM(users) FROM customers</SQL>\n",
    "\n",
    "        <Question>\"How many users do we have for Mobile?\"</Question>\n",
    "        <SQL>SELECT SUM(users) FROM customer WHERE source_medium='Mobile'</SQL>\n",
    "        </examples>\n",
    "          \n",
    "        <Question>{input_question}</Question>\n",
    "        \\n\\n Assistant:\"\"\"\n",
    ")\n",
    "prompt_data= prompt_template.format(schema=schema,input_question = query)\n",
    "print(prompt_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9ff749",
   "metadata": {},
   "source": [
    "With our full prompt ready, let's submit to Claude to see what it comes up with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81addd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = json.dumps({\"prompt\": prompt_data, \"max_tokens_to_sample\": 1500,\"temperature\":0.0})\n",
    "accept = 'application/json'\n",
    "content_type = 'application/json'\n",
    "\n",
    "response = bedrock.invoke_model(body=body, modelId=model_id, accept=accept, contentType=content_type)\n",
    "response_body = json.loads(response.get('body').read())\n",
    "\n",
    "sql = response_body['completion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d643e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71c636d",
   "metadata": {},
   "source": [
    "Let's query our data with this query we generated.\n",
    "First we need to strip the `<SQL>` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ca3a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sql = sql.split('<SQL>')[1].split('</SQL>')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fe7ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = din_sql.query(cleaned_sql)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
